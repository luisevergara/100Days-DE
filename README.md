# 100 Days of Data Engineering

This repository is a collection of my journey through 100 days of learning and practicing various data engineering technologies. The goal of this project is to solidify my understanding of the tools, technologies, and best practices used in the field of data engineering.

## Overview

Data engineering is a crucial aspect of modern data-driven organizations. It involves designing, building, and maintaining the systems and pipelines that enable the efficient collection, storage, processing, and analysis of data from diverse sources. Throughout this 100-day challenge, I will be exploring and hands-on experience with various data engineering concepts, tools, and frameworks.

## Technologies

Some of the key technologies and concepts that will be covered in this repository include:

- Data Warehousing (e.g., AWS Redshift, Google BigQuery, Snowflake)
- Data Lakes (e.g., AWS S3, Google Cloud Storage, Azure Data Lake Storage)
- Data Ingestion and ETL/ELT (e.g., Apache Kafka, Apache NiFi, Talend, AWS Glue)
- Data Processing Frameworks (e.g., Apache Spark, Apache Hadoop, Apache Flink)
- Data Streaming (e.g., Apache Kafka, Apache Flink, AWS Kinesis)
- Data Formats (e.g., CSV, JSON, Parquet, Avro)
- Orchestration and Scheduling (e.g., Apache Airflow, AWS Step Functions, Google Cloud Composer, Mage)
- Data Quality and Monitoring (e.g., Great Expectations, AWS Deequ, Datadog)
- Data Modeling (e.g., Dimensional Modeling, Data Vault, Star Schema)
- Databases (e.g., PostgreSQL, MySQL, MongoDB)
- Cloud Platforms (e.g., AWS, Google Cloud Platform, Microsoft Azure)
- Infrastructure as Code (e.g., Terraform, AWS CloudFormation)
- Containerization and Orchestration (e.g., Docker, Kubernetes)

## Repository Structure

This repository is organized into folders representing each day's progress. Each folder contains code, documentation, and any relevant resources related to the topics or technologies covered on that particular day.